\chapter{Implementierung und Design}
\label{cha:implementation_design}

\textit{In diesem Kapitel wird die Implementierung und das Design der Containerisierung von JULEA erläutert. Zu Beginn wird noch einmal kurz auf den Ist-Zustand eingegangen.}

\section{Ist-Zustand}

Eine JULEA-Entwicklungsumgebung kann aktuell in zweierlei Formen aufgesetzt werden:

Zum einen kann man die nötigen Abhängigkeiten manuell über das Betriebssystem installieren. Dies bring den vorteil mit sich, dass diese automatisiert korrekt installiert werden und man diese üblicherweise auch bereits fertig kompiliert bereitgestellt bekommt. Der Nachteil ist, dass man nur ein begrenztes Spektrum an Versionen zur Verfügung hat und man nicht entscheiden kann, welche Features bei der Abhängigkeit aktiviert oder deaktiviert seien sollen. Des Weiteren ist es durchaus möglich, dass die benötigte Abhängigkeiten nicht in der Paketrepository des Betriebssystems vorhanden sind. 

Zum anderen kann man die Abhängigkeiten über ein mitgeliefertes Skript installieren ("install-dependencies.sh"). Dieses Skript nutzt den Paketmanager "spack" um die Abhängigkeiten zu installieren. Dadurch ist man nicht mehr auf das Paketrepository des Betriebssystems angewiesen und könnte auch spezifische Versionen der Abhängigkeiten installieren. Der Nachteil ist, dass diese Abhängigkeiten kompiliert werden müssen. Dies benötigt Zeit und Rechenleistung. 

JULEA wird bereits mit GitHub-Actions automatisiert kompiliert und getestet. Es gibt bereits eine Pipeline, welche Ubuntu-Container mit den benötigten Abhängigkeiten für JULEA erstellt. Momentan finden dies Container keine Verwendung, in den nachgelagerten CI-Pipelines. 

\todo[inline]{Vergleich von Compilezeiten zwischen unterschiedlichen maschinen}

Um die Entwicklung von JULEA zu erleichtern wäre es sinnvoll eine Betriebssystemagnostische Entwicklungsumgebung zu haben, wo die Abhängigkeiten bereits vorinstalliert sind. Nachfolgend wird erläutert wie dies umgesetzt wurde und welche Containervisualisierungssoftware dafür verwendet wurde.

\section{Benötigte Containerimages}

Um die Containerimages möglichst effizient zu gestalten, ist es sinnvoller mehrere domänenspezifische Containerimages zu erstellen, als ein großes Containerimage, welches dann ggf. Komponenten beinhaltet, welche man nicht benötigt.

Bei Betrachtung der JULEA-Repository fallen zwei Domänen auf. Zu einem ist es eine Entwicklungsumgebung, welche alle optionalen und nicht-optionalen Abhängigkeiten von JULEA zum Kompilieren benötigt, sowie eine Testumgebung, um JULEA zu testen. Außerdem sollte man eine Deploymentumgebung haben, worin eine bereits kompilierte version JULEA mit den benötigten laufzeitabhängigkeiten enthalten ist.

\subsection{Kompilierer}

JULEA und dessen Abhängigkeiten werden gegen 2 Kompilierer getestet und kompiliert. Zum einen GCC und zum anderen CLang.
Es diese Variation sollte auch in den Containerimages abgebildet werden. 

Hierbei gilt allerdings zu beachten, dass stets beide Kompilierer während der Kompilierung verfügbar sind. Welcher Kompilierer verwendet wird, wird durch u. A. die Umgebungsvariable "CC" festgelegt.  

\subsection{Betriebssystem(-Versionen)}

Momentan wird JULEA exklusiv – während der CI-Pipeline – auf Ubuntu kompiliert und getestet. Dabei finden die Ubuntu-LTS-Versionen 20.04, 22.04 und 24.04 Verwendung. Dies wird auch in den Containerimage-Variationen abgebildet. Es werden Containerimages gegen die Ubuntu-Basis-Images 20.04, 22.04 und 24.04 erstellt.

\subsection{Abhängigkeitsquelle}

Des Weiteren gibt es zwei Möglichkeiten um die benötigten Abhängigkeiten, welche JULEA zum Kompilierungszeitpunkt benötigt, zu installieren. Entweder nutzt man das mitgelieferte Skript "install-dependencies.sh", oder man stellt die Abhängigkeiten über das Betriebssystem bereit. Somit muss der Entwicklungs- sowie der Produktivcontainer in jeder Kombination von Betriebssystem, Kompilierer und Abhängigkeits-Installationsmethode vorhanden sein.

\subsection{Entwicklungs- und Produktivcontainer}

Container können verwendet werden, um das Ausrollen von Applikationen zu vereinfachen. Dieses Konzept kann man auch auf die Entwicklungsumgeung erweitern. Somit wird auch eine Containierisierte Entwicklungsumgebung für JULEA erstellt, welche alle Nötigen Abhängigkeiten zum Kompilieren von JULEA enthält.

\subsection{Namensschema}

In diesem Abschnitt wird das Namensschema der Containerimages erläutert und aufgezeigt, welche Containerimages erstellt werden. 

Zuerst werden die produktiven Containerimages vorgestellt und anschließend die Entwicklungscontainerimages. 

\subsubsection{Produktive Containerimages}

Das Namensschema der produktiven Containerimages ist wie folgt: \\
JULEA:\{Kompilierer\}-\{abhängigkeitsquelle\}-\{betriebssystem(version)\}
Somit gibt es folgende produktiven Containerimages:

\begin{multicols}{2}
    \begin{itemize}
        \item JULEA:gcc-system-ubuntu-20.04  
        \item JULEA:gcc-system-ubuntu-22.04  
        \item JULEA:gcc-system-ubuntu-24.04  
        \item JULEA:clang-system-ubuntu-20.04
        \item JULEA:clang-system-ubuntu-22.04
        \item JULEA:clang-system-ubuntu-24.04
        \item JULEA:gcc-spack-ubuntu-20.04   
        \item JULEA:gcc-spack-ubuntu-22.04   
        \item JULEA:gcc-spack-ubuntu-24.04   
        \item JULEA:clang-spack-ubuntu-20.04 
        \item JULEA:clang-spack-ubuntu-22.04 
        \item JULEA:clang-spack-ubuntu-24.04 
    \end{itemize} 
\end{multicols}

Es ist üblich, dass es für ein Image-Tag immer eine "latest" version existiert. Diese wird automatisch ausgewählt, sollte keine spezifische Version angegeben werden.

Als "latest"-Version wird das Image "JULEA:gcc-system-ubuntu-24.04" verwendet. 

\subsubsection{Entwicklungs-Containerimages}

Desweiteren gibt es noch die Entwicklungscontainer. Diese haben das Namensschema: \\
JULEA-dev:\{Kompilierer\}-\{abhängigkeitsquelle\}-\{betriebssystem(version)\}

Somit gibt es folgende Entwicklungscontainer:

\begin{multicols}{2}
    \begin{itemize}
        \item JULEA-dev:system-ubuntu-20.04  
        \item JULEA-dev:system-ubuntu-22.04  
        \item JULEA-dev:system-ubuntu-24.04  
        \item JULEA-dev:gcc-spack-ubuntu-20.04   
        \item JULEA-dev:gcc-spack-ubuntu-22.04   
        \item JULEA-dev:gcc-spack-ubuntu-24.04   
        \item JULEA-dev:clang-spack-ubuntu-20.04 
        \item JULEA-dev:clang-spack-ubuntu-22.04 
        \item JULEA-dev:clang-spack-ubuntu-24.04 
    \end{itemize} 
\end{multicols}

Eine besonderheit ist, dass es für die "system" Variante des Entwickungscontainers keine Kompilierer-Variation gibt. Das liegt daran, dass bei der "system" variation keine Abhängigkeiten kompiliert werden müssen. Außerdem sind stets beide Kompilierer in der Entwicklungsumgebung vorhanden. Somit wären die Kompilierer-Variationen renundant und könnten für Verwirrung sorgen. 

Es ist üblich, dass es für ein Image-Tag immer eine "latest" version existiert. Diese wird automatisch ausgewählt, sollte keine spezifische Version angegeben werden.

Als "latest"-Version wird das Image "JULEA:gcc-system-ubuntu-24.04" verwendet. 

\section{Aufbau der Dockerfiles}

Der de-facto standard um Containerimages zu bauen, ist das Erstellen einer Containerfile (umgansprachlich Dockerfile). Diese werden von verschiedenen OCI-Containervisualisierungssoftwares, wie Docker, Podman, Buildah unterstützt. Ein alternatives Format stellt Apptainer dar, dass sogennante "Apptainer definition file" Dateiformat. Von der benutzung wird abgesehen, da dieses Format nicht sehr weit verbreitet ist und Apptainer die aus der Containerfile herforgehenden OCI-Containerimages unterstützt. Somit erzieht man mit der Containerfile eine hohe Kompatibilität innerhalb der Containerisierungslandschaft, währenddessen man Apptainer unterstützt welche eine weitverbreitete Containerlösung innerhalb des HPC darstellt.

Der generische Aufbau der Stages, welcher hier Anwendung findet, ist wie folgt: 

\begin{figure}[!htbp]
    \centering
    \includesvg[width=400pt]{./figures/modell-containerfile.drawio.svg}
    \caption{Containerfile Stage-Graf}
\end{figure}

\FloatBarrier

\subsection{Stage 0: Basis}

Diese stage dient als Grundlage. Hier wird ausgewählt welches Image als basis verwendet wird, welche Argumente es geben soll und anderweitige grundlegende Konfigurationen. \
Besonders wichtig ist es hier die Argumente zu definieren. Da Argumente, welche nicht in dieser Basis-Stage definiert werden, auch nicht in den anderen Stages verfügbar \cite[Vgl. "Understand how ARG and FROM interact"]{DockerfileReference0100}. Man müsste also die Argumente in jeder Stage – wo man sie benötigt – erneut definieren.  


\subsection{Stage 1: Runtime Abhängigkeiten installieren}

In dieser Stage, welche direkt auf der Basis-Stage aufbaut, werden alle Abhängigkeiten installiert, welche man zur laufzeit benötigt. Dazu gehören Bibliotheken oder andere Programme, welche zur Laufzeit von JULEA benötigt werden.


\subsection{Stage 2: Compiletime Abhängigkeiten installieren}

In dieser Stage, welche direkt auf der Runtime-Stage aufbaut, werden alle Abhängigkeiten installiert welche man exklusiv nur zur Kompilierung von JULEA benötigt. Das Trennen von Stage 1 und Stage 2 ist eine bewusste Entscheidung mit dem Ziel, dass im resultierendem Containerimage nur die Abhängigkeiten enthalten sind, welche auch wirklich für die Ausführung benötigt werden. Dies spart Platz.

\subsection{Stage 3: Konfigurieren und Kompilieren}

In dieser Stage, welche direkt auf der "Compiletime Abhängigkeiten installieren"-Stage aufbaut, wird JULEA konfiguriert und kompiliert. Hier wird u. A. der Kompilierer verwendet, welcher über ein Argument übergeben wird. Hier wird das Kompilat an einen spezifischen Ort bereitgestellt, jedoch noch nicht installiert.

\subsection{Stage 4a: Entwicklungsabhängigkeiten installieren}

Diese stage baut auf der Stage 3 auf, da wir das in Stage 3 erstellte Kompilat nicht benötigen. 
In dieser Stage werden für die Entwicklung nützliche oder benötigte Abhängigkeiten installiert. 

Die Stage 4a wird ausgewählt, wenn man einen JULEA Entwicklungscontainer (JULEA-dev) erstellen möchte. 

\subsection{Stage 4b: Kompilat installieren}

Diese stage baut auf der Stage 1 auf, da wir lediglich die Laufzeitabhängigkeiten benötigen. Anschließend werden die in Stage 4 erstellten Kompilate installiert, indem man sie an den richtigen Ort kopiert.

Die Stage 4b wird ausgewählt, wenn man einen JULEA Entwicklungscontainer (JULEA) erstellen möchte.

\subsection{"System" Containerfile}

Der Layer-Graf für das Containerfile "System" sieht wie folgt aus. 
\begin{figure}[!htbp]
    \centering
    \includesvg[width=400pt]{./figures/modell-system-containerfile.drawio.svg}
    \caption{Containerfile "System" Layer-Graf}
\end{figure}
\FloatBarrier

Man erkennt, dass dieser identisch zum generischen Aufbau ist. Anschließend wird das Dockerfile in den Selben Schritten wie oben beschrieben aufgebaut.

\subsubsection{Stage 0: Basis}

\inputminted[firstline=0,lastline=7]{dockerfile}{./code-examples/Dockerfile.system}

Die Basis-Stage ist kompakt und beinhaltet alle nötigen Argumente, wie die zu verwendende Ubuntu-Version, den "Buildtype" und den Kompilierer. Außerdem wird die Standard-Shell auf "bash" gesetzt. Dies hat u. A. einfluss auf die Shell, indem die "RUN" befehle ausgeführt werden. Es wird später ein Befehl ausgeführt, welcher ohne die verwendung einer bash-shell nicht funktionieren würde.

Außerdem wird das Ubuntu-Versions-Argument zweimal definiert. Es erscheint renundant, ist jedoch nicht anders zu bewerkstelligen. Das Argument wird in Zeile 1 definiert, um es in Zeile 3 zu verwenden. Da in Zeile 3 ein "FROM"-Statement ist, verfällt die Gültigkeit des Arguments. Deshalb wird es in Zeile 4 erneut definiert, um es später in anderen Stages zu verwenden. 

Die Entscheidung alle Argumente in einer Stage "global" zu definieren, hat den Vorteil, dass diese Argumente an einem Ort zu finden sind und man davon ausgehen kann, dass in allen Stages die gleichen Argumente verwendet werden können. Dies macht das Dockerfile übersichtlicher und erleichtert die Wartung. 

\subsubsection{Stage 1: Runtime Abhängigkeiten installieren}

\inputminted[firstline=10,lastline=25]{dockerfile}{./code-examples/Dockerfile.system}

Diese Stage besteht aus einem sehr langen Befehl. Hier werden alle Abhängigkeiten installiert, welche auch zur Laufzeit von JULEA benötigt werden. Zum installieren wird hier der Standard-Paketmanager von Ubuntu "apt-get" verwendet. 

In dieser Stage gibt es einige Unterschiede im Vergleich zu dem wie man die Pakete interaktiv installieren würde. 

Zum einen wird "apt-get update" und "apt-get install" in einem "RUN"-Befehl ausgeführt. Dies ist eine technische Notwendigkeit. Wie bereits im technischen Hintergrund erläutert, nutzt Docker Caching um die Build-Zeit in subsequenten Builds zu verringern. Würde man "apt-get update" und "apt-get install" in zwei "RUN"-Befehlen ausführen, würde "apt-get update" separat von "apt-get install" ausgeführt und gecached werden. Initial würde dies kein Problem darstellen, sobald man allerdings im "apt-get install" Schritt Pakete hinzufügt oder entfernt, würde "apt-get install" nicht mehr gecached sein und müsste neu ausgeführt werden. Da sich "apt-get update" allerdings nicht geändert hat, würde das immernoch gecached sein. Dies würde zu Problemen führen, da die Paketinformationen, welche "apt-get install" benötigt, nicht mehr aktuell sind. Typische symptome sind hierbei, dass Pakete nicht mehr gefungen werden können, da sie nicht mehr im Paketrepository vorhanden sind. Beide Befehle in einem "RUN"-Befehl auszuführen, löst dieses Problem, da bei jeder Änderung der "apt-get install" Zeile, auch "apt-get update" ausgeführt wird.

Des Weiteren wird DEBIAN\_FRONTEND=noninteractive und "--yes" gesetzt. Beide diese Optionen sind notwendig, um "apt-get install" im nicht-interaktiven Modus auszuführen. Ohne diese optionen kann es dazu kommen, dass "apt-get install" nachfragen stellt und eine Nutzereingabe erwartet. Dies ist in einem Dockerfile nicht möglich, da es keine Nutzereingabe gibt.

Außerdem werden die "dev"-Pakete der Varianten installiert. Diese Pakete enthalten nicht nur die Bibliotheken, sondern auch die Header-Dateien, welche für das Kompilieren von JULEA benötigt werden. Es werden also nicht nur die Laufzeitabhängigkeiten installiert, sondern auch teile der Kompilierzeitabhängigkeiten. \
Ubuntu stellt selbstverständlich auch die Pakete als reine Bibliotheken zur Verfügung. Hier besteht allerdings das Problem, dass diese eine spezifische Version im Paketnamen haben. Diese Versionen ändern sich von Ubuntu-Version zu Ubuntu-Version. Somit ist es nicht möglich diese Paketnamen hart zu kodieren. Man müsste diese dynamisch generieren. Dies würde das Containerfile verkomplizieren und die Wartungsintensivität erhöhen. Als Kompromiss werden die "dev"-Pakete installiert und etwas mehr Speicherplatz verbraucht.

\subsubsection{Stage 2: Compiletime Abhängigkeiten installieren}

\inputminted[firstline=28,lastline=43]{dockerfile}{./code-examples/Dockerfile.system}

Wie zu erwarten sieht diese Stage ähnlich zur stage 3 aus. Hier werden wie in Stage 2 auch Abhängigkeiten installiert. 

Allerdings sieht man in Zeile 43 eine Besonderheit. In Ubuntu-Versionen älter als 22.04 ist die installierte Meson Version älter als die von JULEA benötigte Version. Deshalb muss hier die Meson-Version für die älteren Versionen mit pip installiert werden. 

\subsubsection{Stage 3: Konfigurieren und Kompilieren}

\inputminted[firstline=46,lastline=53]{dockerfile}{./code-examples/Dockerfile.system}



Die dazugehörigen Dockerfile sieht wie folgt aus: 

\inputminted{dockerfile}{./code-examples/Dockerfile.system}

\subsection{"Spack" Containerfile}
Der Layer-Graf für das Containerfile "Spack" sieht wie folgt aus.
\begin{figure}[!htbp]
    \centering
    \includesvg[width=400pt]{./figures/modell-spack-containerfile.drawio.svg}
    \caption{Containerfile "Spack" Layer-Graf}
\end{figure}

Die dazugehörige Dockerfile hat folgenden Aufbau.

\inputminted{dockerfile}{./code-examples/Dockerfile.spack}

Bei beiden Dockerfiles fällt auf, dass apt-get update und apt-get install stets im selben "RUN"-Befehl ausgeführt werden. Dies gilt nicht nur für apt-get, sondern auch für andere Paketmanager, welche vor der installation vorgelagerte Schritte ähnlich wie apt-get update benötigen. Sollte man die vorgelagerten schritte in einzelne "RUN" befehle ausführen, hätte das für das erste ausführen von docker build keine Implikationen, da dort der cache noch nicht befüllt wurde. Bei allen folgenden Ausführungen von docker build, kann es nach Anpassung der apt-get install Zeile zu Problemen führen. Dies liegt daran, dass alle vorgelagerten Schritte immer noch im cache liegen, bei dem apt-get install schritt kommt es allerdings zu einem cache-miss und dieser muss neu evaluiert werden. Wenn nun die paketinformationen, welche für apt-get update im layer cache liegen nicht mehr aktuell sind, wird apt-get install fehlschlagen, da es die Pakete, welche beim Mirror angefragt werden nicht mehr gibt. Um dieses Problem zu beheben, sollte man stets apt-get update und apt-get install im selben "RUN"-Befehl ausführen. Dadurch wird bei jeder Anpassung der benötigten Pakete, automatisch auch apt-get update ausgeführt und es kann zu keinen veralteten Paketinformationen kommen. 

\subsection{Zusammenspiel Dockerfile -> Images/Tags}

Welche Dockerfile generiert welche Images/Tags?

\section{Docker Bakefile}

Das Erstellen mehrerer Containerimages aus Containerfiles ohne einen Automatismus, kann sehr aufwendig werden, da man jedes Image einzeln erstellen lassen muss. Insbesondere bei der Erstellung einer großen Menge von Containerimages wie es hier der Fall ist, wäre das manuell Erstellen der Containerimages mit einem zu hohen aufwand verbunden. 

Es gibt eine große Anzahl an Build-Automatisierungs-Tools. Ein sehr bekanntes Programm ist GNU-Make. Eine weitere Möglichkeit wäre auch ein einfaches Shell-Skript, welche die Erstellung automatisiert. Das Erstellen kann außerdem mithilfe von CI-Pipelines vereinfacht werden. GitHub-Actions hat explizite Aktionen, um Containerimages zu erstellen. Mithilfe einer Matrix könnte man somit bei GitHub-Actions alle Containerimages unkompliziert erstellen. Eine weitere option ist, das Benutzen einer "Bakefile", dies ist ein spezieller Dialekt der Hashicorp-Configuration-Language (HCL). Bakefiles sind ein feature von Docker Buildx, um mehrere Containerimages vordefiniert zu erstellen. Es löst also genau das oben beschriebene Problem in einem standardisierten und einfachen Weg. Außerdem hat GitHub-Actions auch eine Action für Docker-Bakefiles, somit lässt es sich auch in die existierende CI/CD-Pipeline integrieren. 

Die Bakefile selber kann in 5 Teile unterteilt werden.

\subsection{Bakefile Header}

In diesem Teil werden Variablen, Gruppen und generische Targets definiert.
\inputminted[firstline=1,lastline=7]{./lexers/docker-bake-lexer.py}{./code-examples/docker-bake.hcl}

Das target "base" ist das basis-target. Hier können alle generellen einstellungen gesetzt werden, ohne, dass man diese in jeder target-definition wiederholen muss. 

Anschließen werden zwei Variablen definiert. 

"BAKE\_IMAGE\_NAME" ist der festgelegte Basisname aller resultierenden Containerimages. Sprich alle Containerimages werden wie folgt benannt: "BAKE\_IMAGE\_NAME*:TAG".

Diese Variable macht es einfach von außen dynamisch festzulegen, wo das image veröffentlicht werden soll. Als default-wert, wird der JULEA-Fork für diese Arbeit verwendet.

"COMMIT\_SHA" wird genutzt, um von außen (üblicherweise von einer CI-Pipeline) den Commit-Hash zu übergeben. Dieser wird dann benutzt, um Containerimages für spezifische Commits zu erstellen. Diese können dann für das einfache debugging von spezifischen Commits zu verwenden.

Zuletzt gibt es die Gruppe "ubuntu". Den Namen braucht man keine weitere Bedeutung zuzuweisen. Diese Gruppe ist lediglich dafür da, um alle folgenden targets in einem Docker Buildx Befehl zu erstellen. Gruppen können von Docker Buildx Bake build wie ein target angesprochen werden. Das heißt, dass man anstelle von mehreren Docker Buildx befehlen, nur einen Befehl ausführen muss, um mehrere targets zu erstellen.

\subsection{Bakefile Target "ubuntu-spack"}

In diesem Target werden die Containerimages für die produktiven JULEA-Container, welche Spack für das Abhängigkeitsmanagement verwenden, definiert.

\inputminted[firstline=9,lastline=33]{./lexers/docker-bake-lexer.py}{./code-examples/docker-bake.hcl}

Dieses Target – wie alle anderen Targets auch – ist ein "Matrix-Target". Das bedeutet, dass dieses Target eigentlich mehrere Targets darstellt. Dies wird mit dem Attribut "matrix" gemacht. Das Attribut selber hat 2 unterattribute: "version", welches die zu verwendenden Ubuntu-Major-Versionen angibt, und "compilers", welche die zu verwendenden Kompilierer angibt.
Die "versions" und "compilers" Matrix-Variables werden in dem Target wie andere Variablen auch benutzt. 
Bei Matrix-Targets ist es wichtig, dass der name für jede Matrix-Variante eindeutig ist. Das bedeutet, dass man die Matrix-Variablen in den Namen einbauen muss. Dies sieht man im Attribut "name"

Nach dem Matrix-Attribut kommt das "args"-Attribut. Mit diesem werden Attribute an die Dockerfile weitergegeben. In diesem Fall die zu verwendende Ubuntu-Version, welcher Kompilierer Spack verwenden soll, und mit welchem Kompilierer JULEA kompiliert werden soll. 

Anschließend werden die zu generierenden Container-Tags definiert. Zum einen wird der generelle Tag für diese JULEA-Variante erstellt. Dies ist der erste eintrag. Zum anderen wird das gleiche Image noch einmal als Tag mit dem Commit-Hash erstellt.

Danach wird definiert welche Dockerfile und welches Target innerhalb der Dockerfile gebaut werden soll. 

Zuletzt wird noch das caching aktiviert, um die hier besonders langen Compile-Zeiten in folgenden Builds zu minimieren. Das hier konfigurierte caching ist das GitHub-Actions caching, welches das unkomplizierte caching von Docker Builds in GitHub-Actions ermöglicht.

\subsection{Bakefile Target "ubuntu-system"}

Dieses target ist fast identisch zum "ubuntu-spack" target. Der einzige Unterschied ist, dass hier nicht gecached wird, da das Bauen dieser Container ohne Caching bereits sehr schnell ist und die Cache-Größe von GitHub-Actions begrenzt ist. Außerdem wird hier das Dockerfile "Dockerfile.system" verwendet und die Tags haben andere Namen. Es wird des Weiteren kein Spack compiler Argument an das Dockerfile übergeben, da das Dockerfile.system bereits die Abhängigkeiten über das Betriebssystem installiert und kein Spack verwendet wird. 

\inputminted[firstline=35,lastline=50]{./lexers/docker-bake-lexer.py}{./code-examples/docker-bake.hcl}

\subsection{Bakefile Target "ubuntu-latest"}

Dieses Target sehr ähnlich zum "ubuntu-system" target. Hier wird allerdings der "latest"-Tag erzeugt. 
Der rest ist identisch zum "ubuntu-system" target. Es wäre auch möglich diesen mit dem "ubuntu-system" target zu kombinieren, allerdings würde das die Lesbarkeit des Bakefiles verringern, da man nun spezielle Funktionen und konditionelle Anweisungen im Bakefile-Target verwenden müsste.

\inputminted[firstline=52,lastline=62]{./lexers/docker-bake-lexer.py}{./code-examples/docker-bake.hcl}

\subsection{Bakefile Target "ubuntu-dev-container"}

Dieses Target ist das letzte Target und erstellt die Entwicklungscontainer. Diese Container sind identisch zum Spack-Produktionscontainer, allerdings wird hier ein anderes Target verwendet ("JULEA\_dependncies"). Dieses Target hat JULEA noch nicht kompiliert und lediglich die nötigen Abhängigkeiten mit Spack installiert.

\inputminted[firstline=64]{./lexers/docker-bake-lexer.py}{./code-examples/docker-bake.hcl}

\section{CI}

Um das Erstellen der Container noch komfortabler zu machen, werden diese mithilfe einer CI-Pipeline erstellt und automatisch in ein Container-Repository geladen. Da das JULEA-Projekt auf GitHub gehostet wird und bereits GitHub-Actions als CI-/CD-Lösung verwendet, wird auf diese Pipeline aufgebaut und GitHub-Actions verwendet.

Hierbei wird die Pipeline um die Funktionalität des Container-Erstellens sowie veröffentlichen erweitert. Des Weiteren werden in den nachgelagerten Test-Pipelines auch die erstellten Container verwendet.

Die CI-Pipeline wird außerdem die erstellten OCI-Containerimages in Apptainer-Images umwandeln und diese als Artefakte bereitstellen.


