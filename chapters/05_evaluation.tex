\chapter{Evaluation} \label{cha:evaluation}

\section{Benchmarkumgebung}

Eines der möglichen Variablen, welches das Benchmarkergebnis substantiell beeinflussen kann,  ist die Benchmarkumgebung. Die Benchmarkumgebung wird hier als drei verschiedene Komponenten betrachtet. Das eine ist die Hardware, auf welcher der Benchmark ausgeführt wird. Das zweite ist das unterliegende Betriebssystem und dessen Komponenten auf welchen der Benchmark ausgeführt wird und das dritte sind die anderen Prozesse, welche sich auf dem System ressourcen mit den Benchmarkprozesse\(n\) teilen. Bevor detailierter auf den eigentlichen Benchmark eingegangen wird, werden die drei Komponenten der Benchmarkumgebung genauer betrachtet.

\subsection{Hardware}

Der Benchmark wurde auf dem HPC-Cluster der Otto-von-Guericke-Universität Magdeburg ausgeführt. Das Cluster selber besteht aus mehreren "Knoten" (engl. Nodes), welche wie eigenständige Computer agieren. Es gibt Knoten mit verschiedenen Hardware-Spezifikationen. Der Benchmark wurde stets auf einem Knoten mit folgender Hardware-Spezifikation ausgeführt:

\begin{itemize}
    \item CPU: 1x AMD Epyc 7443 @ $2,85\text{GHz}$
    \item RAM: 16GB DDR4
    \item /home, /data, /opt/spack: CephFS
    \item /\*\* (auch /tmp): $1\text{TB}$ SATA-SSD  
\end{itemize}

In beiden Fällen lag das Kompilat im /home Verzeichnis, währenddessen die Speicherorte von JULEA auf /tmp definiert waren. Insbesondere ist zu beachten, dass für Apptainer das /tmp Verzeichnis des "Host" Systems mit einem Bind-Mount in den Container eingebunden wurde. Somit ist hier die Umgebung beider Ausführungsarten, so weit wie möglich, identisch.

Zusammenfassend lässt sich sagen, dass die Hardware in beiden Fällen identisch ist und es somit zu keiner Verzerrung der Benchmarkergebnisse durch unterschiedliche Hardwarekonfigurationen geben sollte. 

\subsection{Betriebssystem und Betriebssystemkomponenten}

Das Betriebssystem, auf welchem der Benchmark ausgeführt wurde, ist CentOS 8. Die Betriebssystemversion im Container ist im gegensatz dazu Ubuntu 24.04. Dieser Unterschied ist allerdings nicht relevant. Der größte unterschied zwischen den beiden Betriebssystemen würden die unterschiedlichen Library-Versionen sein, da in beiden fällen der CentOS 8 Linux-Kernel verwendet wird. Die unterschiedlichen Library-Versionen spielen allerdings keine Rolle, da hier die libraries mit Spack (gcc) kompiliert wurden. Somit gibt es hierbei auch keine Unterschiede. Es wurde in beiden fällen GCC 12 verwendet. Unterschiede in der Performance durch einen deutlich neueren Kompilierer sind somit auch ausgeschlossen. 

Hier sind die beiden Umgebungen somit auch in den relevanten Komponenten identisch. Somit sollte hier auch keine Verzerrungen der Benchmarkergebnisse durch unterschiedliche Betriebssystem-Konfigurationen geben.

\subsection{Andere Prozesse}

Der Benchmark musste sich die Umgebung nur mit dem Betriebssystem und dessen Prozessen teilen. Somit ist es unwahrscheinlich, dass das System durch andere Prozesse stark beeinflusst wurde und es dadurch zu Milderungen der Benchmarkergebnissen kam.

Hier gibt es keine Unterschiede zwischen den beiden Ausführungsarten und somit sollten sich die Benchmarkergebnisse nicht durch andere Prozesse auf dem System unterschiedlich verhalten.


Abschließend lässt sich sagen, dass die Benchmarkergebnisse sehr wahrscheinlich nicht durch unterschiedliche Benchmarkumgebungen verzerrt werden können, da die Benchmarkumgebungen in den relevanten Komponenten (Hardware, Betriebssystem und Betriebssystemkomponenten und Andere Prozesse) identisch sind.   

\section{Ausführung des Benchmarks}

Für die Ausführung des Benchmarks auf dem HPC-Cluster müssen die Ressourcen allokiert und die Ausführung der Programme gescheduled werden. Auf dem HPC-Cluster der OvGU wird das über das Slurm-System realisiert. Die ausführlichen SLURM-Skripte sind im Anhang zu finden.

Für das SLURM-Skript wurde folgende Parameter gesetzt

\begin{itemize}
    \item --constraint=epyc3: Begrenzt die Allokation von Nodes auf den oben genannten CPU
    \item --exclusive: Node wird exklusiv für den Benchmark reserviert
    \item --time=0-2:30:00: Maximale ausführungsdauer von $2,5h$
    \item --mem=16GB: 16GB RAM auf dem Node werden reserviert
    \item --array=1-10: zehnfache Ausführung des Benchmarks
    \item --output=./res/apptainer/benchmark-\%a.out: Datei in welches das STDOUT geschrieben wird
    \item --error=./res/apptainer/benchmark-\%a.err: Datei in welches das STDERR geschrieben wird 
\end{itemize}

Die SLURM-Skripte, welche für die Ausführung des Benchmarks verwendet wurden, sind im Anhang in \cref{cha:appendix-benchmarkskripts} vorhanden.

\section{Erörterung der Benchmarkarkmetriken}

Um genauer zu verstehen, welche Faktoren einen Einfluss auf die Benchmarkergebnisse haben, ist es wichtig die Bedeutung der Benchmarkmetriken, sowie dessen unterliegende Technologien zu verstehen. 
Die hier verwendeten Benchmarks sind in JULEA integriert und in verschiedene Kategorien unterteilt.

Diese Kategorien (Benchmarkmetriken) sind stets in einem spezifischen Format angegeben, dieses Format ist einem POSIX-Pfad ähnlich. Die Benchmarkmetriken sind in verschiedene (Unter-)Kategorien unterteilt. Die Struktur einer Benchmarkmetrik ist somit /Kategorie/Unterkategorie-0/../Unterkategorie-n-1/Unterkategorie-n.

Die von JULEA erstellten Benchmarkergebnisse liefern ein Paar Kennzahlen. Die aussagekräftigste Kennzahl, um die Performance zweier Systeme zu vergleichen ist die Anzahl der Operationen pro Sekunde (operations/s). Eine Operation ist eine Ausführung der beschriebenen Benchmarkmetrik. So ist "/kv/get" eine "get" anfrage an das KV-Backend von JULEA. 

Folgend werden die Benchmarkmetriken nach ihren Kategorien unterteilt und erläutert.

\subsection{Objektspeicher}

Objektspeicher wird unter den Benchmarkmetriken, welche "/object" als Präfix besitzen, gebenchmarkt.

Dabei gibt es eine Differenzierung zwischen "/object/object" und "/object/distributed-object". "/object/object" repräsentiert die Operationen auf Objekten, welche auf dem lokalen Dateisystem gespeichert sind. "/object/distributed-object" repräsentiert die Operationen auf Objekten, welche auf einem verteilten Dateisystem gespeichert sind. 

\subsection{Key-Value}

Die Benchmark zu der Key-Value-Komponente von JULEA haben "/kv" als Prefix. 

Hierbei werden alle üblichen Operationen einer Key-Value-Datenbank gebenchmarkt. Dazu gehören das Einfügen, Aktualisiere, Löschen, sowie Lesen von Key-Value-Paaren.

\subsection{Item-Store}

Der Item-Store – alle Benchmarkmetriken, welche mit "/item" beginnen – repräsentieren die von JULEA unterstützten Operationen auf dem Item-Store.

\subsection{Datenbank}

Die Datenbank-Benchmarkmetriken haben "/db" als Präfix. Hier werden übliche Datenbankoperationen wie das Einfügen, Löschen, Aktualisieren von Einträgen, sowie das Erstellen und Löschen von Schemas gebenchmarkt.

\subsection{Ausgeschlossene Benchmarkmetriken}

Die Benchmarkmetriken Message ("/message"), Cache ("/cache") und Background Operation ("/background") wurden in dieser Auswertung nicht berücksichtigt. Die Benchmarkmetriken Message und Background wiesen eine sehr hohe Varianz auf, was eine Aussage mit Konfidenz nicht möglich macht. Die Cache Benchmarkmetrik ist für den Vergleich beider Lösungsansätze nicht relevant, da Apptainer keine Virtualisierung des System-Memories vornimmt. Es wird transparent auf den Zwischenspeicher des Hostsystems zugegriffen.   

\section{Vor-Auswertung der Benchmarkergenisse}

Um die Benchmarkergebinsse effektiv auswerten zu können ist im ersten Schritt wichtig sich die Verteilung der Messwerte anzusehen. Ein Boxplot, ist eine populäre Möglichkeit Verteilungen zu analysieren \cite[Vgl. 1]{majawExploringDataDistributions2023}. Nachfolgend werden die Verteilungen der nativen sowie containerisierten Benchmarkergebnisse an einem Beispiel betrachtet. Ein vollumfängliche Betrachtung aller Verteilungen wird nicht vorgenommen, da die Verteilungen sich alle ähnlich verhalten.

Die gesammten Benchmarkergebnisse sind im Anhang in \cref{cha:appendix-benchmarkdaten} vorhanden.

\pagebreak

\subsection{Native}

Die Verteilungen der Messwerte des Benchmarks, welche Nativ auf dem Host-System ausgeführt wurden, lassen sich aus der folgenden Grafiken entnehmen:

\includesvg[width=1\linewidth]{benchmark/vis/compressed/boxplots/system/object/boxplot.svg}
\FloatBarrier

Es ist erkenntlich, dass die Verteilungen der Messwerte oftmals verzerrt sind, und es einige Ausreißer gibt. Bei dieser Benchmarkbetrachtung soll keine genauere Betrachtung der Extremwerte stattfinden. Darum ist die Betrachtung des Medianwertes als statistisches Mittel hier Sinnvoller \cite[Vgl. 15f.]{stengelStatistikUndAufbereitung2011}. 

Des Weiteren liegen die gemittelten Messwerte der einzelnen Benchmarkmetriken zum Teil weit auseinander, was zur Folge hat, dass der grafische Vergleich zwischen den verschiedenen Ausführungsarten des Benchmarks mit absoluten Werten nicht sehr aussagekräftig ist. Darum wird der Vergleich der verschiedenen Ausführungsarten des Benchmarks mithilfe des "Speedups" berechnet. Dieser betrachtet die relative Verbesserung zwischen zwei Messwerten: 


\begin{equation}
S_{\frac{\text{ops}}{\text{s}}} = \frac{O_{\text{containerized}}}{O_{\text{native}}}
\end{equation}


\subsection{Containerized}

Die Schlüsse, welche bei der Betrachtung der nativen Ausführung des Benchmarks gezogen wurden, lassen sich auch auf die Containerized-Ausführung des Benchmarks übertragen. Die Messergebnisse des Benchmarks, welche in einem Apptainer-Container ausgeführt wurden, lassen sich aus der folgenden Grafik entnehmen:

\includesvg[width=1\linewidth]{benchmark/vis/compressed/boxplots/apptainer/object/boxplot.svg}


Es ist bereits bei Betrachtung der Boxplots ersichtlich, dass beide Ergebnisse größtenteils ähnlich sind. Um noch genauer die Performance-Unterschiede zu erkennen, wird im nächsten Schritt der Speedup $S_(\text{system}, \text{apptainer})$ für die einzelnen Benchmarkmetriken betrachtet.

\pagebreak

\section{Vergleich der Benchmarkergebnisse}

Um einen besseren Überblick über den Unterschied zwischen beiden Benchmarkergebnissen zu erhalten, wird der errechnete Speedup um -1 verschoben. Das hat zur Folge, dass anstelle vom Wert 1, nun der Wert 0 identische Performance bedeutet. 
Negative Werte bedeuten, dass die containerisierte Ausführung des Benchmarks schlechter ist als die Native Ausführung. Positive Werte bedeuten, dass die containerisierte Ausführung des Benchmarks besser ist als die Native Ausführung. 

Die übergeordneten Kategorien der Benchmarkmetriken, werden nachfolgend einzeln betrachtet.

\subsection{Objektspeicher}

\begin{figure}[H]
    \centering
    \includesvg[width=1\linewidth]{benchmark/vis/compressed/differences/object/difference_operations.svg}
    \caption{Speedup der Benchmarkergebnisse für den Objektspeicher}
    \label{fig:speedup_object}
\end{figure}

\FloatBarrier

Als Erstes wird der Speedup der Benchmarkmetrik "object" betrachtet. Hier ist zu erkennen, dass die Containerisierte ausführung des Benchmarks stets zwischen $1\%$ bis $3\%$ langsamer ist. Die Benchmarkmetrik "object" misst dabei die Performance der Objektverwaltung. Diese Objektverwaltung läuft bei diesem Benchmark über das Dateisystem. Somit ist es naheliegend, dass, in diesem Fall, das Dateisystem ein Bottleneck sein könnte. Allerdings ist diese sehr konsistente Performance-Verschlechterung im Vergleich zur nativen Ausführung auf den ersten Blick nicht erklärbar, da beide Ausführungsweisen auf das gleiche Verzeichnis im Host-Dateisystem zugegriffen haben (/tmp). 

Im Fall von Apptainer wurde das Verzeichnis mithilfe eines bind-mounts in den Container eingebunden. Für die Containertechnologie Docker gab es bereits eine Performance-Analyse von bind-mounts und es konnte keine Signifikaten Performance-Einbußen festgestellt werden \cite[Vgl. 4]{dordevicFileSystemPerformance2022}. 

Somit lässt sich darauf schließen, dass die bind-mounts nicht die Ursache für die Performance-Einbußen sind. Bei Apptainer besteht jedoch noch die Besonderheit, dass die Container-Images ein SquashFS-Dateisystem verwenden. Dieses Dateisystem wird beim Start des Containers in das Host-Dateisystem eingehängt. 

Hier gibt es bei Apptainer zwei möglichkeiten, wie dieses Containerimage eingehangen wird. \\
Entweder wird das Image mit dem Kernel-SquashFS-Treiber eingehangen, was priviligierte Rechte benötigt, oder es wird mithilfe des SquashFS-FUSE-Treibers eingehangen. Hierfür werden keine privilegierten Rechte benötigt. Allerdings ist laut eigenen aussagen von Apptainer die Performance des FUSE-Treibers schlechter als die des Kernel-Treibers. Somit könnte dies eine mögliche Ursache für die Performance-Einbußen sein \cite{apptainerSecurityApptainerApptainer}.  

\subsection{Key-Value (lmdb)}

\begin{figure}[H]
    \centering
    \includesvg[width=0.7\linewidth]{benchmark/vis/compressed/differences/kv/difference_operations.svg}
    \caption{Speedup der Benchmarkergebnisse für Key-Value}
    \label{fig:speedup_kv}
\end{figure}

\FloatBarrier

Die Key-Value-Komponente von Julea ("kv"), hingegen weist kaum eine Veränderung der Performance zwischen der nativen und containerisierten Ausführung auf. Die Performance geht zwar leicht zurück, allerdings ist der rückgang deutlich unter $1\%$ uns es kann nicht ausgeschlossen werden, dass dies nur eine Messungenauigkeit ist. Eine Metrik, sticht jedoch heraus. Die Metrik "kv/get" weist eine Performance-Senkung von mehr als $1,7\%$ auf. Diese Metrik weißt auf die Performance des lesend innerhalb der Key-Value-Datenbank hin. In diesem fall wird LMDB als lösung im hintergrund verwendet. Die Datenbank selber nutzt memory-mapping, um lese und schreibzugriffe möglichst performant zu machen. Hierbei sollte es zu keinem signifikanten Overhead kommen, da das Memory-Mapping durch den Kernel verwaltet wird und die Datenbank-Datei(en) selber direkt auf dem Host-Dateisystem liegen. Somit wird der SquashFS-FUSE-Treiber während der Ausführung des Benchmarks umgangen und es kann zu keinem Performance-Verlust kommen. Desweiteren würde ein solcher Performance-Verlust – sollte es ihn dennoch geben – auch bei der "put"-Benchmarkmetrik zu erkennen sein. Da dies nicht der Fall ist, kann davon ausgegangen werden, dass die Performance-Einbußen bei der "kv/get"-Metrik nicht durch das Dateisystem verursacht werden. Es könnte sein, dass die Julea-Spezifische implementierung der interaktion mit der Datenbank, in der Containerisierten Umgebung, hier zu Performance-Einbußen führt. 

\subsection{Item-Store}

\begin{figure}[H]
    \centering
    \includesvg[width=1\linewidth]{benchmark/vis/compressed/differences/item/difference_operations.svg}
    \caption{Speedup der Benchmarkergebnisse für den Item-Store}
    \label{fig:speedup_item}
\end{figure}

\FloatBarrier

Der Item-Store ist im vergleich zu den vorherigen Benchmarkmetriken anders. Er baut auf beiden Implementierungen auf.

\subsubsection{Item}

Zum einen wird für die Operationen "/item/\{item, collection\}/create", sowie "/item/\{item, collection\}/delete" auf die mechanisem "/kv/put", "/kv/delete" aufgebaut, was den ähnlich insignifikanten Speedup zu den beiden "kv"-Metriken erklärt. 
Die Operationen "/item/item/read" und "/item/item/write" hingegen, bauen auf den Objekt-Mechanismen "/object/object/read" und "/object/object/write" auf. Bei "/item/item/write"

Bei den befehlen "/item/item/read" und "/item/item/write" ist wie zu erwarten auch ein Performance-Verlust – wie bei den object-read/write Ergebnissen – zu erkennen. Bei "/item/item/read" ist der Performance-Verlust um etwas mehr als $1\%$ geringer als bei "/object/object/read". Bei "/item/item/write" ist der Performance-Verlust um etwa $0.5\%$ geringer als bei "/object/object/write". Da es sich hierbei um geringe Unterschiede handelt, ist davon auszugehen, dass dies Messungenauigkeiten sind. 

Die Metrik "/item/item/delete" hat einen positiven Speedup. Dieser ist jedoch sehr gering ($<0.25\%$). Diese Metrik baut auf den "/kv/delete" Mechanismus auf. "/kv/delete" hat einen geringen Performance-Verlust aufgewiesen. Der Unterschied zwischen den beiden Metriken ist sehr gering. Er liegt unterhalb von $0.5\%$. Da es sich hierbei um geringe Unterschiede handelt, ist davon auszugehen, dass dies Messungenauigkeiten sind. Das gleiche gilt auch für "/item/collection/delete".

Wesentlich auffälliger ist hingegen die Metrik "/item/collection/create". Diese hat Performance-Zunahmen von fast $2\%$. Um dieses Ergebnis besser zu verstehen werden nachstehend die Messwertverteilungen der beiden Benchmarkergenisse analysiert. 

\begin{figure}[H]
    \centering
    \includesvg[width=1\linewidth]{benchmark/vis/compressed/differences/comparisons/run_to_run_distribution__item_collection_create.svg}
    \caption{Speedup der Benchmarkergebnisse für den Item-Store}
    \label{fig:speedup_item_collection_create}
\end{figure}

\FloatBarrier

Es ist ersichtlich, dass die Benchmark-Ergebnisse von der nativen Ausführung deutlich konsistenter sind als die der containerisierten Ausführung. Mehr als die Hälfte der nativen Ergebnisse liegen hier zwischen 5200 und 5300 Operationen/s. Allerdings ist die Streumenge der nativen Ergebnisse deutlich größer. 

Die containerisierten Ergebnisse sind wie bereits angemerkt deutlich inkonsistenter und die Streumenge ist hier deutlich geringer. Die Verteilung der Messwerte ist – im Vergleich zur nativen Ausführung – in die positive Richtung verzerrt.  

Trotz dieser Unterschiede sieht man allerdings hier auch wie identisch die beiden Benchmarkergebnisse im Kern sind. Während bei der nativen Ausführung die meisten Ergebnisse zwischen 5200 und 5300 Operationen/s liegen, liegen die meisten containerisierten Ergebnisse zwischen 5200 und ca. 5325 Operationen/s. Das ist in Anbetracht der Insgesammt 10 Iterationen des Benchmarks pro Ausführungsweise nicht signifikant genug um von einer signifikanten Performance-Änderung zu sprechen. Es handelt sich hierbei sehr wahrscheinlich um Messungenauigkeiten. 


\subsection{Datenbank (SQLite)}

\begin{figure}[H]
    \centering
    \includesvg[width=1\linewidth]{benchmark/vis/compressed/differences/db/difference_operations.svg}
    \caption{Speedup der Benchmarkergebnisse für Datenbanken}
    \label{fig:speedup_db}
\end{figure}

\FloatBarrier

Die Speedups der Datenbank-Benchmarkergebnisse für die Metriken "/db/entry/delete", "/db/entry/insert" und "/db/iterator/get-simple" sind deutlich unterhalb von $1\%$, die werte sind zu gering um von einer signifikanten Performance-Änderung zu sprechen. 

Die Metrik "/db/entry/update" hingegen ist beinahe $3\%$ performanter. Um hier eine genauere Aussage zu treffen, wird im nächsten Schritt die Verteilung der Messwerte der Benchmarkergebnisse analysiert.

\begin{figure}[H]
    \centering
    \includesvg[width=1\linewidth]{benchmark/vis/compressed/differences/comparisons/run_to_run_distribution__db_entry_update.svg}
    \caption{Verteilung Benchmarkergebnisse für "/db/entry/update"}
    \label{fig:mdist_db_entry_update}
\end{figure}

\FloatBarrier

Es ist ersichtlich, dass die containerisierte Ausführung wesentlich konsistentere Ergebnisse erzieht hat. 
Die nativen Ergebnisse sind etwas inkonsitenter und haben einen Ausreißer nach unten. 

Allerdings sieht man auch hier, dass sich die Ergebnisse von der containierisierten und nativen Lösung zwischen ca. 5100 und 5250 Operationen/s mehrheitlich befinden. 

Dies ist ein weiteres Beispiel dafür, wie ähnlich die beiden Benchmarkergebnisse sind. Die Messwerte liegen so weit bei einander, dass man in Anbetracht der geringen Messwertanzahl keinen Signifikanten Speedup feststellen kann.

\begin{figure}[H]
    \centering
    \includesvg[width=1\linewidth]{benchmark/vis/compressed/differences/comparisons/run_to_run_distribution__db_schema_create.svg}
    \caption{Verteilung Benchmarkergebnisse für "/db/schema/create"}
    \label{fig:mdist_db_schema_create}
\end{figure}

\begin{figure}[H]
    \centering
    \includesvg[width=1\linewidth]{benchmark/vis/compressed/differences/comparisons/run_to_run_distribution__db_schema_delete.svg}
    \caption{Verteilung Benchmarkergebnisse für "/db/schema/delete"}
    \label{fig:mdist_db_schema_delete}
\end{figure}

Für "/db/schema\{create, delete\}" ist anschließend auch ersichtlich, dass sich bei betrachtung der Messwertvertilung kein signifikanter Speedup festgestellt werden kann. Die Ergebnisse liegen soweit beieinander, dass nicht auszuschließen ist, dass es sich um Messungenauigkeiten handelt.

\FloatBarrier

\subsection{Fazit der Benchmarkergebnisse}

Insgesammt lässt sich ein deutlichen Bild erkennen. Die Benchmarkergebnisse unterscheiden sich nur sehr geringfügig. Selbst da wo es zu Performance-Verlusten kommt, sind diese sehr gering und liegen unterhalb von $3\%$. \
Viele der Benchmarkergebnisse sind effektiv – unter berücksichtigung der Messungenaugikeiten – identisch.

Dies ist kein Überraschendes Ergebniss, es wurde bereits in vielen anderen Publikationen ein Ähnlichen verhalten wie hier festgestellt.  \todo{Zitat}

Sollte man starken Gebrauch von Dateisystemfunktionen machen, so kann es zu Performance-Einbußen kommen. Allerdings ist in den meisten Fällen ein Performance-Verlust von unter $3\%$ akzeptabel. 