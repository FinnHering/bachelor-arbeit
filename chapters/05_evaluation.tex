\chapter{Evaluation}

\section{Vor-Auswertung der Benchmarkergenisse}

Um die Benchmarkergebinsse effektiv auswerten zu können ist im ersten Schritt wichtig sich die Verteilung der Messwerte anzusehen. Um Schlüsse aus den Messwerten ziehen zu können, ist es wichtig die Verteilung der Messwerte zu kennen. Ein Boxplot, ist eine populäre Möglichkeit Verteilungen zu analysieren \cite[Vgl. 1]{majawExploringDataDistributions2023}. Nachfolgend werden die Verteilungen der nativen sowie containerisierten Benchmarkergebnisse betrachtet. Um die Boxplots übersichtlich zu gestalten, werden für jede unterkategorie der Benchmarkergebnisse (z.B. /db, /kv, etc.) ein Boxplot erstellt. Desweiteren werden messwerte, welche sich beim verglich von nativen und containerisierten Benchmarkergebnissen gleich verhalten und in der gleichen Kategorie liegen, in einem Boxplot zusammengefasst. Die vollständige Auswertung ist im Appendix zu finden. 
\todo[inline]{Hier nochmal auf die Appendix verweisen und vollständige Auswertung im Appendix anhängen}

\subsection{Native}

Die Verteilungen der Messwerte des Benchmarks, welche Nativ auf dem Host-System ausgeführt wurden, lassen sich aus der folgenden Grafiken entnehmen:

\includesvg[width=1\linewidth]{benchmark/vis/compressed/boxplots/system/db/boxplot.svg}
\includesvg[width=1\linewidth]{benchmark/vis/compressed/boxplots/system/item/boxplot.svg}
\includesvg[width=1\linewidth]{benchmark/vis/compressed/boxplots/system/kv/boxplot.svg}
\includesvg[width=1\linewidth]{benchmark/vis/compressed/boxplots/system/object/boxplot.svg}
\includesvg[width=1\linewidth]{benchmark/vis/compressed/boxplots/system/message/boxplot.svg}


\FloatBarrier

Es ist erkenntlich, dass mit steigenden durchschnittlichen Operationen/s einer Benchmarkmetrik sich auch – in den meisten Fällen – die Varianz der Messwerte erhöht. Außerdem ist erkenntlich, dass die Verteilungen der Messwerte oftmals verzerrt sind, und es einige Ausreißer gibt. Bei dieser Benchmarkbetrachtung soll keine genauere Betrachtung der Extremwerte stattfinden. Darum ist die Betrachtung des Medianwertes als statistisches Mittel hier Sinnvoller \cite[Vgl. 15f.]{stengelStatistikUndAufbereitung2011}. 

Des Weiteren liegen die gemittelten Messwerte der einzelnen Benchmarkmetriken zum Teil weit auseinander, was zur Folge hat, dass der grafische Vergleich zwischen den verschiedenen Ausführungsarten des Benchmarks mit absoluten Werten nicht sehr aussagekräftig ist. Darum wird der Vergleich der verschiedenen Ausführungsarten des Benchmarks mithilfe des "Speedups" berechnet. Dieser betrachtet die relative Verbesserung zwischen zwei Messwerten: 


\begin{equation}
S_{\frac{\text{ops}}{\text{s}}} = \frac{O_{\text{containerized}}}{O_{\text{native}}}
\end{equation}


\subsection{Containerized}

Die Schlüsse, welche bei der Betrachtung der nativen Ausführung des Benchmarks gezogen wurden, lassen sich auch auf die Containerized-Ausführung des Benchmarks übertragen. Die Messergebnisse des Benchmarks, welche in einem Apptainer-Container ausgeführt wurden, lassen sich aus der folgenden Grafik entnehmen:


\includesvg[width=1\linewidth]{benchmark/vis/compressed/boxplots/apptainer/db/boxplot.svg}
\includesvg[width=1\linewidth]{benchmark/vis/compressed/boxplots/apptainer/item/boxplot.svg}
\includesvg[width=1\linewidth]{benchmark/vis/compressed/boxplots/apptainer/kv/boxplot.svg}
\includesvg[width=1\linewidth]{benchmark/vis/compressed/boxplots/apptainer/object/boxplot.svg}
\includesvg[width=1\linewidth]{benchmark/vis/compressed/boxplots/apptainer/message/boxplot.svg}


Es ist bereits bei Betrachtung der Boxplots ersichtlich, dass beide Ergebnisse größtenteils ähnlich sind. Um noch genauer die Performance-Unterschiede zu erkennen, wird im nächsten Schritt der Speedup $S_(\text{system}, \text{apptainer})$ für die einzelnen Benchmarkmetriken betrachtet.

\section{Vergleich der Benchmarkergebnisse}

Um einen besseren Überblick über den Unterschied zwischen beiden Benchmarkergebnissen zu erhalten, wird der errechnete Speedup um -1 verschoben. Das hat zur folge, dass anstelle vom Wert 1, nun der Wert 0 identische Performance bedeutet. 
Negative Werte bedeuten, dass die Containerisierte Ausführung des Benchmarks schlechter ist als die Native Ausführung. Positive Werte bedeuten, dass die Containerisierte Ausführung des Benchmarks besser ist als die Native Ausführung. 

\todo[inline]{Benchmark-Grafik noch etwas splitten.}
\begin{figure}
    \centering
    \includesvg[width=1\linewidth]{benchmark/vis/compressed/differences/difference_operations.svg}
    \caption{Speedup der Benchmarkergebnisse}
    \label{fig:speedup}
\end{figure}

\FloatBarrier

\subsection{Objektspeicher}

Als Erstes wird der Speedup der Benchmarkmetrik "object" betrachtet. Hier ist zu erkennen, dass die Containerisierte ausführung des Benchmarks stets zwischen $1\%$ bis $3\%$ langsamer ist. Die Benchmarkmetrik "object" misst dabei die Performance der Objektverwaltung. Diese Objektverwaltung läuft bei diesem Benchmark über das Dateisystem. Somit ist es naheliegend, dass, in diesem Fall, das Dateisystem ein Bottleneck sein könnte. Allerdings ist diese sehr konsistente Performance-Verschlechterung im Vergleich zur nativen Ausführung auf dem ersten Blick nicht erklärbar, da beide Ausführungsweisen auf das gleiche Verzeichnis im Host-Dateisystem zugegriffen haben (/tmp). Im Fall von Apptainer wurde das Verzeichnis mithilfe eines bind-mounts in den Container eingebunden. Allerdings gab es für die Containertechnologie Docker bereits eine Performance-Analyse von bind-mounts und es konnte keine Signifikaten Performance-Einbußen festgestellt werden \cite[Vgl. 4]{dordevicFileSystemPerformance2022}. Somit lässt sich darauf schließen, dass die bind-mounts nicht die Ursache für die Performance-Einbußen sind. Allerdings gibt es bei Apptainer noch die besonderheit, dass die Container-Images ein SquashFS-Dateisystem verwenden. Dieses Dateisystem wird beim Start des Containers in das Host-Dateisystem eingehängt. Hier gibt es bei Apptainer zwei möglichkeiten, wie dieses Containerimage eingehangen wird. Entweder wird das Image mit dem Kernel-SquashFS-Treiber eingehangen, was allerdings priviligierte Rechte benötigt, oder es wird mithilfe des SquashFS-FUSE-Treibers eingehangen. Hierfür werden keine privilegierten Rechte benötigt. Allerdings ist laut eigenen aussagen von Apptainer die Performance des FUSE-Treibers schlechter als die des Kernel-Treibers. Somit könnte dies eine mögliche Ursache für die Performance-Einbußen sein \cite{apptainerSecurityApptainerApptainer}.  

\subsection{Key-Value (lmdb)}

Die Key-Value-Komponente von Julea ("kv"), hingegen weist kaum eine Veränderung der Performance zwischen der nativen und containerisierten Ausführung auf. Die Performance geht zwar leicht zurück, allerdings ist der rückgang deutlich unter $1\%$ uns es kann nicht ausgeschlossen werden, dass dies nur eine Messungenauigkeit ist. Eine Metrik, sticht jedoch heraus. Die Metrik "kv/get" weist eine Performance-Senkung von mehr als $1,7\%$ auf. Diese Metrik weißt auf die Performance des lesend innerhalb der Key-Value-Datenbank hin. In diesem fall wird LMDB als lösung im hintergrund verwendet. Die Datenbank selber nutzt memory-mapping, um lese und schreibzugriffe möglichst performant zu machen. Hierbei sollte es zu keinem signifikanten Overhead kommen, da das Memory-Mapping durch den Kernel verwaltet wird und die Datenbank-Datei(en) selber direkt auf dem Host-Dateisystem liegen. Somit wird der SquashFS-FUSE-Treiber während der Ausführung des Benchmarks umgangen und es kann zu keinem Performance-Verlust kommen. Desweiteren würde ein solcher Performance-Verlust – sollte es ihn dennoch geben – auch bei der "put"-Benchmarkmetrik zu erkennen sein. Da dies nicht der Fall ist, kann davon ausgegangen werden, dass die Performance-Einbußen bei der "kv/get"-Metrik nicht durch das Dateisystem verursacht werden. Es könnte sein, dass die Julea-Spezifische implementierung der interaktion mit der Datenbank, in der Containerisierten Umgebung, hier zu Performance-Einbußen führt. 

\subsection{Item-Store}

Der Item-Store ist im vergleich zu den vorherigen Benchmarkmetriken anders. Er baut auf beiden Implementierungen auf.
\todo[inline]{Mal schauen, ob man das auch in dem Paper zu JULEA findet, dann hier zitieren. Ansonsten auf den JULEA-Source-Code verweisen.}

\subsubsection{Item}

Zum einen wird für die Operationen "/item/item/create", sowie "/item/item/delete" auf die mechanisem "/kv/put", "/kv/delete" aufgebaut, was den ähnlich insignifikanten Speedup zu den beiden "kv"-Metriken erklärt. 
Die Operationen "/item/item/read" und "/item/item/write" hingegen, bauen auf den Objekt-Mechanismen "/object/object/read" und "/object/object/write" auf. Bei "/item/item/write"

\subsubsection{Collection}

\dots


\FloatBarrier

\subsection{Datenbank}

\dots


\subsection{Cache}

\dots

\subsection{Nachrichten}

\dots



\section{Hintergrund-Operationen}

\dots



