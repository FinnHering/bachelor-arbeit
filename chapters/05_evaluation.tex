\chapter{Evaluation}

\section{Erörterung der Benchmarkarkmetriken}

Um genauer zu verstehen, welche faktoren einen Einfluss auf die Benchmarkergebnisse haben, ist es wichtig die bedeutung der Benchmarkmetriken, sowie dessen unterliegende Technologien zu verstehen. Die Benchmarkmetriken sind stets in einem spezifischen Format angegeben, dieses Format ist einem POSIX-Pfad ähnlich. Die Benchmarkmetriken sind in verschiedene (Unter-)Kategorien unterteilt. Die Struktur einer Benchmarkmetrik ist somit /Kategorie/Unterkategorie-0/../Unterkategorie-n-1/Unterkategorie-n.

Folgend werden die Benchmarkmetriken nach ihren Kategorien unterteilt und erläutert.

\subsection{Objektspeicher}

Objektspeicher wird unter den Benchmarkmetriken, welche "/object" als Präfix besitzen, gebenchmarkt.

Dabei gibt es eine Differenzierung zwischen "/object/object" und "/object/distributed-object". "/object/object" repräsentiert die Operationen auf Objekten, welche auf dem lokalen Dateisystem gespeichert sind. "/object/distributed-object" repräsentiert die Operationen auf Objekten, welche auf einem verteilten Dateisystem gespeichert sind. 

\subsection{Key-Value}

Die Benchmark zu der Key-Value-Komponente von JULEA haben "/kv" als Prefix. 

Hierbei werden alle üblichen Operationen einer Key-Value-Datenbank gebenchmarkt. Dazu gehören das Einfügen, Aktualisiere, Löschen, sowie Lesen von Key-Value-Paaren.

\subsection{Item-Store}

Der Item-Store – alle Benchmarkmetriken, welche mit "/item" beginnen – repräsentieren die von JULEA unterstützten Operationen auf dem Item-Store.

\subsection{Datenbank}

Die Datenbank-Benchmarkmetriken haben "/db" als Präfix. Hier werden übliche Datenbankoperationen wie das Einfügen, Löschen, Aktualisieren von Einträgen, sowie das Erstellen und Löschen von Schemas gebenchmarkt.

\subsection{Ausgeschlossene Benchmarkmetriken}

Die Benchmarkmetriken Message ("/message"), Cache ("/cache") und Background Operation ("/background") wurden in dieser Auswertung nicht berücksichtigt. Die Benchmarkmetriken Message und Background wiesen eine sehr hohe Varianz 


\section{Vor-Auswertung der Benchmarkergenisse}

Um die Benchmarkergebinsse effektiv auswerten zu können ist im ersten Schritt wichtig sich die Verteilung der Messwerte anzusehen. Um Schlüsse aus den Messwerten ziehen zu können, ist es wichtig die Verteilung der Messwerte zu kennen. Ein Boxplot, ist eine populäre Möglichkeit Verteilungen zu analysieren \cite[Vgl. 1]{majawExploringDataDistributions2023}. Nachfolgend werden die Verteilungen der nativen sowie containerisierten Benchmarkergebnisse betrachtet. Um die Boxplots übersichtlich zu gestalten, werden für jede unterkategorie der Benchmarkergebnisse (z.B. /db, /kv, etc.) ein Boxplot erstellt. Desweiteren werden messwerte, welche sich beim verglich von nativen und containerisierten Benchmarkergebnissen gleich verhalten und in der gleichen Kategorie liegen, in einem Boxplot zusammengefasst. Die vollständige Auswertung ist im Appendix zu finden. 
\todo[inline]{Hier nochmal auf die Appendix verweisen und vollständige Auswertung im Appendix anhängen}

\subsection{Native}

Die Verteilungen der Messwerte des Benchmarks, welche Nativ auf dem Host-System ausgeführt wurden, lassen sich aus der folgenden Grafiken entnehmen:

\includesvg[width=1\linewidth]{benchmark/vis/compressed/boxplots/system/db/boxplot.svg}
\includesvg[width=1\linewidth]{benchmark/vis/compressed/boxplots/system/item/boxplot.svg}
\includesvg[width=1\linewidth]{benchmark/vis/compressed/boxplots/system/kv/boxplot.svg}
\includesvg[width=1\linewidth]{benchmark/vis/compressed/boxplots/system/object/boxplot.svg}
\includesvg[width=1\linewidth]{benchmark/vis/compressed/boxplots/system/message/boxplot.svg}

\FloatBarrier

Es ist erkenntlich, dass mit steigenden durchschnittlichen Operationen/s einer Benchmarkmetrik sich auch – in den meisten Fällen – die Varianz der Messwerte erhöht. Außerdem ist erkenntlich, dass die Verteilungen der Messwerte oftmals verzerrt sind, und es einige Ausreißer gibt. Bei dieser Benchmarkbetrachtung soll keine genauere Betrachtung der Extremwerte stattfinden. Darum ist die Betrachtung des Medianwertes als statistisches Mittel hier Sinnvoller \cite[Vgl. 15f.]{stengelStatistikUndAufbereitung2011}. 

Des Weiteren liegen die gemittelten Messwerte der einzelnen Benchmarkmetriken zum Teil weit auseinander, was zur Folge hat, dass der grafische Vergleich zwischen den verschiedenen Ausführungsarten des Benchmarks mit absoluten Werten nicht sehr aussagekräftig ist. Darum wird der Vergleich der verschiedenen Ausführungsarten des Benchmarks mithilfe des "Speedups" berechnet. Dieser betrachtet die relative Verbesserung zwischen zwei Messwerten: 


\begin{equation}
S_{\frac{\text{ops}}{\text{s}}} = \frac{O_{\text{containerized}}}{O_{\text{native}}}
\end{equation}


\subsection{Containerized}

Die Schlüsse, welche bei der Betrachtung der nativen Ausführung des Benchmarks gezogen wurden, lassen sich auch auf die Containerized-Ausführung des Benchmarks übertragen. Die Messergebnisse des Benchmarks, welche in einem Apptainer-Container ausgeführt wurden, lassen sich aus der folgenden Grafik entnehmen:


\includesvg[width=1\linewidth]{benchmark/vis/compressed/boxplots/apptainer/db/boxplot.svg}
\includesvg[width=1\linewidth]{benchmark/vis/compressed/boxplots/apptainer/item/boxplot.svg}
\includesvg[width=1\linewidth]{benchmark/vis/compressed/boxplots/apptainer/kv/boxplot.svg}
\includesvg[width=1\linewidth]{benchmark/vis/compressed/boxplots/apptainer/object/boxplot.svg}
\includesvg[width=1\linewidth]{benchmark/vis/compressed/boxplots/apptainer/message/boxplot.svg}


Es ist bereits bei Betrachtung der Boxplots ersichtlich, dass beide Ergebnisse größtenteils ähnlich sind. Um noch genauer die Performance-Unterschiede zu erkennen, wird im nächsten Schritt der Speedup $S_(\text{system}, \text{apptainer})$ für die einzelnen Benchmarkmetriken betrachtet.

\section{Vergleich der Benchmarkergebnisse}

Um einen besseren Überblick über den Unterschied zwischen beiden Benchmarkergebnissen zu erhalten, wird der errechnete Speedup um -1 verschoben. Das hat zur folge, dass anstelle vom Wert 1, nun der Wert 0 identische Performance bedeutet. 
Negative Werte bedeuten, dass die Containerisierte Ausführung des Benchmarks schlechter ist als die Native Ausführung. Positive Werte bedeuten, dass die Containerisierte Ausführung des Benchmarks besser ist als die Native Ausführung. 

Die übergeordneten Kategorien der Benchmarkmetriken, werden nachfolgend einzeln betrachtet.

\subsection{Objektspeicher}

\begin{figure}
    \centering
    \includesvg[width=1\linewidth]{benchmark/vis/compressed/differences/object/difference_operations.svg}
    \caption{Speedup der Benchmarkergebnisse für den Objektspeicher}
    \label{fig:speedup_object}
\end{figure}

\FloatBarrier

Als Erstes wird der Speedup der Benchmarkmetrik "object" betrachtet. Hier ist zu erkennen, dass die Containerisierte ausführung des Benchmarks stets zwischen $1\%$ bis $3\%$ langsamer ist. Die Benchmarkmetrik "object" misst dabei die Performance der Objektverwaltung. Diese Objektverwaltung läuft bei diesem Benchmark über das Dateisystem. Somit ist es naheliegend, dass, in diesem Fall, das Dateisystem ein Bottleneck sein könnte. Allerdings ist diese sehr konsistente Performance-Verschlechterung im Vergleich zur nativen Ausführung auf dem ersten Blick nicht erklärbar, da beide Ausführungsweisen auf das gleiche Verzeichnis im Host-Dateisystem zugegriffen haben (/tmp). Im Fall von Apptainer wurde das Verzeichnis mithilfe eines bind-mounts in den Container eingebunden. Allerdings gab es für die Containertechnologie Docker bereits eine Performance-Analyse von bind-mounts und es konnte keine Signifikaten Performance-Einbußen festgestellt werden \cite[Vgl. 4]{dordevicFileSystemPerformance2022}. Somit lässt sich darauf schließen, dass die bind-mounts nicht die Ursache für die Performance-Einbußen sind. Allerdings gibt es bei Apptainer noch die besonderheit, dass die Container-Images ein SquashFS-Dateisystem verwenden. Dieses Dateisystem wird beim Start des Containers in das Host-Dateisystem eingehängt. Hier gibt es bei Apptainer zwei möglichkeiten, wie dieses Containerimage eingehangen wird. Entweder wird das Image mit dem Kernel-SquashFS-Treiber eingehangen, was allerdings priviligierte Rechte benötigt, oder es wird mithilfe des SquashFS-FUSE-Treibers eingehangen. Hierfür werden keine privilegierten Rechte benötigt. Allerdings ist laut eigenen aussagen von Apptainer die Performance des FUSE-Treibers schlechter als die des Kernel-Treibers. Somit könnte dies eine mögliche Ursache für die Performance-Einbußen sein \cite{apptainerSecurityApptainerApptainer}.  

\subsection{Key-Value (lmdb)}


\begin{figure}
    \centering
    \includesvg[width=1\linewidth]{benchmark/vis/compressed/differences/kv/difference_operations.svg}
    \caption{Speedup der Benchmarkergebnisse für Key-Value}
    \label{fig:speedup_kv}
\end{figure}

\FloatBarrier

Die Key-Value-Komponente von Julea ("kv"), hingegen weist kaum eine Veränderung der Performance zwischen der nativen und containerisierten Ausführung auf. Die Performance geht zwar leicht zurück, allerdings ist der rückgang deutlich unter $1\%$ uns es kann nicht ausgeschlossen werden, dass dies nur eine Messungenauigkeit ist. Eine Metrik, sticht jedoch heraus. Die Metrik "kv/get" weist eine Performance-Senkung von mehr als $1,7\%$ auf. Diese Metrik weißt auf die Performance des lesend innerhalb der Key-Value-Datenbank hin. In diesem fall wird LMDB als lösung im hintergrund verwendet. Die Datenbank selber nutzt memory-mapping, um lese und schreibzugriffe möglichst performant zu machen. Hierbei sollte es zu keinem signifikanten Overhead kommen, da das Memory-Mapping durch den Kernel verwaltet wird und die Datenbank-Datei(en) selber direkt auf dem Host-Dateisystem liegen. Somit wird der SquashFS-FUSE-Treiber während der Ausführung des Benchmarks umgangen und es kann zu keinem Performance-Verlust kommen. Desweiteren würde ein solcher Performance-Verlust – sollte es ihn dennoch geben – auch bei der "put"-Benchmarkmetrik zu erkennen sein. Da dies nicht der Fall ist, kann davon ausgegangen werden, dass die Performance-Einbußen bei der "kv/get"-Metrik nicht durch das Dateisystem verursacht werden. Es könnte sein, dass die Julea-Spezifische implementierung der interaktion mit der Datenbank, in der Containerisierten Umgebung, hier zu Performance-Einbußen führt. 

\subsection{Item-Store}

\begin{figure}
    \centering
    \includesvg[width=1\linewidth]{benchmark/vis/compressed/differences/item/difference_operations.svg}
    \caption{Speedup der Benchmarkergebnisse für den Item-Store}
    \label{fig:speedup_item}
\end{figure}

\FloatBarrier

Der Item-Store ist im vergleich zu den vorherigen Benchmarkmetriken anders. Er baut auf beiden Implementierungen auf.
\todo[inline]{Mal schauen, ob man das auch in dem Paper zu JULEA findet, dann hier zitieren. Ansonsten auf den JULEA-Source-Code verweisen.}

\subsubsection{Item}

Zum einen wird für die Operationen "/item/\{item, collection\}/create", sowie "/item/\{item, collection\}/delete" auf die mechanisem "/kv/put", "/kv/delete" aufgebaut, was den ähnlich insignifikanten Speedup zu den beiden "kv"-Metriken erklärt. 
Die Operationen "/item/item/read" und "/item/item/write" hingegen, bauen auf den Objekt-Mechanismen "/object/object/read" und "/object/object/write" auf. Bei "/item/item/write"

Bei den befehlen "/item/item/read" und "/item/item/write" ist wie zu erwarten auch ein Performance-Verlust – wie bei den object-read/write Ergebnissen – zu erkennen. Bei "/item/item/read" ist der Performance-Verlust um etwas mehr als $1\%$ geringer als bei "/object/object/read". Bei "/item/item/write" ist der Performance-Verlust um etwa $0.5\%$ geringer als bei "/object/object/write". Da es sich hierbei um geringe Unterschiede handelt, ist davon auszugehen, dass dies Messungenauigkeiten sind. 

Die Metrik "/item/item/delete" hat einen positiven Speedup. Dieser ist jedoch sehr gering ($<0.25\%$). Diese Metrik baut auf den "/kv/delete" Mechanismus auf. "/kv/delete" hat einen geringen Performance-Verlust aufgewiesen. Der Unterschied zwischen den beiden Metriken ist sehr gering. Er liegt unterhalb von $0.5\%$. Da es sich hierbei um geringe Unterschiede handelt, ist davon auszugehen, dass dies Messungenauigkeiten sind. Das gleiche gilt auch für "/item/collection/delete".

Wesentlich auffälliger ist hingegen die Metrik "/item/collection/create". Diese hat Performance-Zunahmen von fast $2\%$. Um dieses Ergebnis besser zu verstehen werden nachstehend die Messwertverteilungen der beiden Benchmarkergenisse analysiert. 

\begin{figure}
    \centering
    \includesvg[width=1\linewidth]{benchmark/vis/compressed/differences/comparisons/run_to_run_distribution__item_collection_create.svg}
    \caption{Speedup der Benchmarkergebnisse für den Item-Store}
    \label{fig:speedup_item}
\end{figure}

\FloatBarrier

Es ist ersichtlich, dass die Benchmark-Ergebnisse von der nativen Ausführung deutlich konsistenter sind als die der containerisierten Ausführung. Mehr als die Hälfte der nativen Ergebnisse liegen hier zwischen 5200 und 5300 Operationen/s. Allerdings ist die Streumenge der nativen Ergebnisse deutlich größer. 

Die containerisierten Ergebnisse sind wie bereits angemerkt deutlich inkonsistenter und die Streumenge ist hier deutlich geringer. Die Verteilung der Messwerte ist – im Vergleich zur nativen Ausführung – in die positive Richtung verzerrt.  

Trotz dieser Unterschiede sieht man allerdings hier auch wie identisch die beiden Benchmarkergebnisse im Kern sind. Während bei der nativen Ausführung die meisten Ergebnisse zwischen 5200 und 5300 Operationen/s liegen, liegen die meisten containerisierten Ergebnisse zwischen 5200 und ca. 5325 Operationen/s. Das ist in Anbetracht der Insgesammt 10 Iterationen des Benchmarks pro Ausführungsweise nicht signifikant genug um von einer signifikanten Performance-Änderung zu sprechen. Es handelt sich hierbei sehr wahrscheinlich um Messungenauigkeiten. 


\subsection{Datenbank (sqlite)}

\begin{figure}
    \centering
    \includesvg[width=1\linewidth]{benchmark/vis/compressed/differences/db/difference_operations.svg}
    \caption{Speedup der Benchmarkergebnisse für Datenbanken}
    \label{fig:speedup_db}
\end{figure}

\FloatBarrier

Die Speedups der Datenbank-Benchmarkergebnisse für die Metriken "/db/entry/delete", "/db/entry/insert" und "/db/iterator/get-simple" sind deutlich unterhalb von $1\%$, die werte sind zu gering um von einer signifikanten Performance-Änderung zu sprechen. 

Die Metrik "/db/entry/update" hingegen ist beinahe $3\%$ performanter. Um hier eine genauere Aussage zu treffen, wird im nächsten Schritt die Verteilung der Messwerte der Benchmarkergebnisse analysiert.

\begin{figure}
    \centering
    \includesvg[width=1\linewidth]{benchmark/vis/compressed/differences/comparisons/run_to_run_distribution__db_entry_update.svg}
    \caption{Verteilung Benchmarkergebnisse für "/db/entry/update"}
    \label{fig:mdist_db_entry_update}
\end{figure}

\FloatBarrier

Es ist ersichtlich, dass die containerisierte Ausführung wesentlich konsistentere Ergebnisse erzieht hat. 
Die nativen Ergebnisse sind etwas inkonsitenter und haben einen Ausreißer nach unten. 

Allerdings sieht man auch hier, dass sich die Ergebnisse von der containierisierten und nativen Lösung zwischen ca. 5100 und 5250 Operationen/s mehrheitlich befinden. 

Dies ist ein weiteres Beispiel dafür, wie ähnlich die beiden Benchmarkergebnisse sind. Die Messwerte liegen so weit bei einander, dass man in Anbetracht der geringen Messwertanzahl keinen Signifikanten Speedup feststellen kann.


Für "/db/schema\{create, delete\}" ist anschließend auch ersichtlich, dass sich bei betrachtung der Messwertvertilung kein signifikanter Speedup festgestellt werden kann. Die Ergebnisse liegen soweit beieinander, dass nicht auszuschließen ist, dass es sich um Messungenauigkeiten handelt.

\begin{figure}
    \centering
    \includesvg[width=1\linewidth]{benchmark/vis/compressed/differences/comparisons/run_to_run_distribution__db_schema_create.svg}
    \caption{Verteilung Benchmarkergebnisse für "/db/schema/create"}
    \label{fig:mdist_db_schema_create}
\end{figure}

\begin{figure}
    \centering
    \includesvg[width=1\linewidth]{benchmark/vis/compressed/differences/comparisons/run_to_run_distribution__db_schema_delete.svg}
    \caption{Verteilung Benchmarkergebnisse für "/db/schema/delete"}
    \label{fig:mdist_db_schema_delete}
\end{figure}

\FloatBarrier

\subsection{Cache}

\todo[inline]{Vlcht. Cache komplett Entfernen?}

\begin{figure}
    \centering
    \includesvg[width=1\linewidth]{benchmark/vis/compressed/differences/cache/difference_operations.svg}
    \caption{Speedup der Benchmarkergebnisse für Caching}
    \label{fig:speedup_cache}
\end{figure}

\FloatBarrier

Der Speedup von der cache Benchmarkmetrik ist nicht signifikant. Es wird davon ausgegangen, dass die Engebnisse Identisch sind. Dies ist auch realistisch, da Apptainer keine Virtualisierung des System-Memories vornimmt. Es wird transparent auf den Zwischenspeicher des Hostsystems zugegriffen.


\subsection{Fazit}

Insgesammt lässt sich ein deutlichen Bild erkennen. Die Benchmarkergebnisse unterscheiden sich nur sehr geringfügig. Selbst da wo es zu Performance-Verlusten kommt, sind diese sehr gering und liegen unterhalb von $3\%$. \
Viele der Benchmarkergebnisse sind effektiv – unter berücksichtigung der Messungenaugikeiten – identisch.

Dies ist kein Überraschendes Ergebniss, es wurde bereits in vielen anderen Publikationen ein Ähnlichen verhalten wie hier festgestellt.  \todo{Zitat}

Sollte man starken Gebrauch von Dateisystemfunktionen machen, so kann es zu Performance-Einbußen kommen. Allerdings ist in den meisten Fällen ein Performance-Verlust von unter $3\%$ akzeptabel. 